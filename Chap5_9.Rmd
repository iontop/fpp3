---
title: "Chap5_9"
author: "J.H AHN"
date: '2021 12 9 '
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(fpp3)

knitr::opts_chunk$set(echo = TRUE)
```

## 5.9 Evaluating distributional forecast accuracy


`accuracy()`함수로 구한 결과는 측정점의 예측 정확도를 보여주는 것으로 분포 예측을 평가하기 위해서는 다른 측정 지표를 사용해야 한다.  




### Quantile scores

Chap 5.8에서 사용한 예시를 가져와 naive method로 예측한 prediction intervals = 80% 구간을 나타내면 아래와 같다.  


```{r}
# Re-index based on trading days
google_stock <- 
    gafa_stock %>%
    filter(Symbol == "GOOG", year(Date) >= 2015) %>%
    mutate(day = row_number()) %>%
    update_tsibble(index = day, regular = TRUE)

# Filter the year of interest
google_2015 <- 
    google_stock %>% 
    filter(year(Date) == 2015)

google_2015

```


```{r}
google_jan_2016 <- 
    google_stock %>% 
    filter(yearmonth(Date) == yearmonth("2016 Jan"))

google_jan_2016

```



```{r}
google_fit <- google_2015 %>%
    model(
        Mean = MEAN(Close),
        Naive = NAIVE(Close),
        DRIFT = RW(Close ~ drift()))

google_fit

```


```{r}
google_fc <- google_fit %>%
    forecast(google_jan_2016)

google_fc

```


```{r}
google_fc %>%
    filter(.model == "Naive") %>% 
    autoplot(bind_rows(google_2015, google_jan_2016),
             level = 80) +
    labs(y = "$US",
         title = "Google closing stock prices")

```


prediction intervals의 하한값은 10번째 백분위수(또는 0.1 분위수)이므로 실제 값은 시간의 약 10% 하한값보다 낮고 약 90%의 하한값보다 높을 것으로 예상된다.  
실제 값과 이 백분위수를 비교할 때는 실제값이 아래보다 위에 있을 가능성이 더 높다는 사실을 감안해야 한다.  

백분위수 예측 시 미래 시점 $t$에서의 예측 백분위수 확률(quantile forecast with probability) $p$를 $f_{p,t}$로 나타낼 수 있다.  
관측값 $y_t$는 확률 $p$의 $f_{p,t}$보다 작을 것으로 기대할 수 있다.  
예를 들어 10번째 백분위수는 $f_{0.1,t}$로 표현될 수 있다.  
만약 $y_t$를 시점$t$의 관측값으로 나타낸다면 백분위 점수(Quantile Score)는 아래와 같다.  

$$Q_{p,t} = \begin{cases}
  2(1 - p) \big(f_{p,t} - y_{t}\big), & \text{if $y_{t} < f_{p,t}$}\\
  2p \big(y_{t} - f_{p,t}\big), & \text{if $y_{t} \ge f_{p,t}$} \end{cases}$$
  
  

이 식의 그래프는 핀볼게임의 핀볼의 궤적과 유사하기 때문에 "핀볼 손실 함수(pinball loss function)"이라고도 불린다.  
2의 승수는 종종 생략되지만 포함시키면 해석이 조금 더 쉬워진다.  
낮은 $Q_{p,t}$을 가질 수록 더 좋은(better) 분위수 추정을 하고 있음을 나타낸다.  


Quantile score는 절대값 오차(absolute error)와 같은 것으로 볼 수 있다.  
$p=0.5$라면, quantile score $Q_{0.5,t}$는 절대값 오차와 같다.  
다른 $p$값에서 $"error" (y_{t}-f_{p,t})$의 양수 혹은 음수일 가능성을 포함시켜 가중치 준다.  
만약 $p>0.5, Q_{p,t}$라면, 관측치가 추정된 분위수 보다 작을 때보다 관측치가 추정된 분위수 보다 클 때 더 큰 페널티를 준다.  
$p<0.5$에서는 이와 반대이다.  



위의 구글 주가 그래프에서 one-step-adhead 10% qunatile forecast (2016년 1월 4일)는 $f_{0.1,t}=744.54$이고 관측치는 $y_{t}=741.84$이다. 그러므로  


$$Q_{0.1,t} = 2(1-0.1) (744.54 - 741.84) = 4.86$$


로 나타낼 수 있고, 이는 `accuracy()`함수의 `quantile_score()` argument를 이용해서 쉽게 계산이 가능하다.  

```{r}
google_fc %>% 
    filter(.model == "Naive", Date == "2016-01-04") %>% 
    accuracy(google_stock, list(qs = quantile_score), probs = 0.1)


```




### Winkler Score


종종 백분위수보다 prediction intervals을 평가하는 것이 더 필요할 때가 있는데 이 때는 Winkler score를 사용한다.  
만약 시간$t$에서의 $100(1-\alpha)%$ prediction intervals을 $[\ell_{\alpha,t}, u_{\alpha,t}]$로 표현한다면, Winkler score는 관측치가 intervals 바깥에 있다면 intervals 범위에 페널티를 더하는 방식으로 정의된다.  

$$W_{\alpha,t} = \begin{cases}
  (u_{\alpha,t} - \ell_{\alpha,t}) + \frac{2}{\alpha} (\ell_{\alpha,t} - y_t) & \text{if } y_t < \ell_{\alpha,t} \\
  (u_{\alpha,t} - \ell_{\alpha,t})   & \text{if }  \ell_{\alpha,t} \le y_t \le u_{\alpha,t} \\
  (u_{\alpha,t} - \ell_{\alpha,t}) + \frac{2}{\alpha} (y_t - u_{\alpha,t}) & \text{if } y_t > u_{\alpha,t}.
\end{cases}$$


prediction intervals 범위 내에 있는 관측치의 Winkler socre는 단순하게 intervals의 길이로 나타난다.  
따라서 낮은 점수는 prediction intervals이 좁을 경우에 나타난다.  
그러나 관측값이 intervals을 벗어나게 되면 페널티가 적용되어 intervals에서 벗어난 거리에 비례하여 페널티가 적용된다.  


Prediction intervals은 항상 $\ell_{\alpha,t} = f_{\alpha/2,t}$와 $u_{\alpha,t} = f_{1-\alpha/2,t}$로 설정된 백분위수를 이루어진다.  
만약 백분위 점수(quantile scores)를 $\alpha$로 나눈다면 아래와 같은 Winkler score를 얻을 수 있다.  

$$W_{\alpha,t} = (Q_{\alpha/2,t} + Q_{1-\alpha/2,t})/\alpha$$


2016년 1월 4일 구글 주가를 one-step-ahead 80%로 예측한 결과는 $744.54 ~ $773.22였고, 실제 주가는 $741.84였다.  
그러므로 Winkler score는 아래와 같이 계산된다.  


$$W_{\alpha,t} = (773.22 - 744.54) + \frac{2}{0.2} (744.54 - 741.84)  =
   55.68$$


이는 `accuracy()`함수의 `winkler_score()` argument를 이용해서 쉽게 계산이 가능하다. 


```{r}
google_fc %>% 
    filter(.model == "Naive", Date == "2016-01-04") %>% 
    accuracy(google_stock, list(qs = winkler_score), level = 80)


```




### Continuous Ranked Probability Score


특정 분위수나 예측구간 보다 전체 예측 분포에 대한 평가를 하고 싶을 때는 연속 순위 확률점수(CRPS, Continuous Ranked Probability Score)로 $p$의 모든 값에 대해 분위수 점수를 평균화 할 수 있다.  (Gneiting & Katzfuss, 2014)  

구글 주가 예시에서 test set의 모든 날짜에 대한 평균 CRPS값을 계산할 수 있다.  
CRPS값은 확률에 가중치를 주고 계산된 전체 예측 분포의 가중 절대 오차(weighted absolute error)와 비슷하다.  

아래 결과를 보면 random walk(with drift) method나 mean method보다 naive method 예측의 분포가 더 좋은 것을 정량적으로 확인 할 수 있다.  

```{r}
google_fc %>% 
    accuracy(google_stock, list(crps = CRPS)) %>% 
    arrange(crps)

```


### Scale-free comparisons using skill scores

포인트 예측과 마찬가지로 분포 예측 정확도도 다양한 scale에 관계없이 비교할 수 있으면 유용하다.  
포인트 예측의 경우 scale에 관계없이 비교하기 위해 scaled error(예를 들어 MASE, RMSSE)와 같은 개념을 쓴다.  
다른 방법으로는 skill score를 사용하는 방법이 있는데 이는 포인트 예측과 분포 예측 정확도 모두 사용할 수 있다.  

skill score를 사용하여 몇몇 benchmark method와 관련있는 예측 정확도를 계산할 수 있다.  
예를 들어 naive method를 benchmark로 사용하고 drift method과 비교한다면 CRPS skill score를 아래와 같이 쓸 수 있다.  

$$\frac{\text{CRPS}_{\text{Naïve}} - \text{CRPS}_{\text{Drift}}}{\text{CRPS}_{\text{Naïve}}}$$


이 식을 이용하면 drift method가 naive method보다 얼마나 더 개선되었는지 CRPS를 통해 알 수 있다.  
이것도 `accuracy()`함수를 이용해서 쉽게 계산이 가능하다. 

```{r}
google_fc %>% 
    accuracy(google_stock, list(skill = skill_score(CRPS))) %>% 
    arrange(desc(skill))

```


benchmark가 되는 naive method의 값은 0으로 나타나고 나머지 두 method에 대한 값이 계산된다.  
나머지 두 method는 navie method보다 CRPS가 낮으므로 음수로 나타난다.  
이 결과로 보면 CRPS기준으로 drift method는 naive method보다 26.6% 더 나쁘다고 평가할 수 있다.  


`skill_score()` argument는 `fable` object가 아니더라도 CRPS를 계산해 준다.  
데이터가 계절성을 가지고 있을 경우 benchmark는 SNAIVE(seasonal naive method)가 사용된다.  
benchmark와 비교 시에는 `accuracy()`함수에 들어가는 training set이 동일한 것이 되도록 하는 것이 중요하다.  


`skill_score()` argument는 모든 정확도 측정에 사용할 수 있다.  
예를 들어 `skill_score(MSE)`라 쓰면 MSE를 비교해준다. 그리고 test set가 안정적으로 오차를 계산할 수 있을만큼 (특히 분모 계산시 )충분히 커야 한다.  
이러한 이유로 scale에 영향을 받지 않는 MASE 또는 RMSSE가 포인트 예측 정확도 비교 시 선호된다.  




*


