---
title: "Chap7_5"
author: "J.H AHN"
date: '2021 12 16 '
output: html_document
---

```{r setup, include=FALSE}
library(fpp3)

knitr::opts_chunk$set(echo = TRUE)
```

## 7.5 Selecting predictors

예측변수가 많을 때 회귀모델에 사용할 예측변수를 선택하기 위해서는 전략이 필요하다.  

권장되지 않는 일반적인 방법은 특정 predictor에 대한 forecast variable을 plot해보고 특별한 관계가 없으면 하나씩 지워나가는 방법이다.  
이러한 방법은 다른 predictor의 효과로 인해 scatterplot에서 관계여부를 확인할 수 없는 경우가 있기 때문에 쓸모가 없다.  

또 다른 쓸모없지만 일반적으로 많이 하는 방식은 모든 예측 변수에 대해 다중 선형 회귀를 수행한 후 p-value가 0.05보다 큰 모든 변수를 지우는 것이다.  
우선 통계적 유의성이 항상 예측값을 지시하는 것은 아니고 예측이 목적이 아니더라도 둘 이상의 예측변수가 서로 상관관계를 가지면 잘못된 p-value를 얻을 수 있기 때문에 이 또한 좋은 방법이 아니다. (Chap7_8 참조)  


위와 같은 방법 대신 예측 정확도 측정값을 사용하는 것이 좋다.  
이번 장에서는 5가지 방법을 제시할 것이다. 
정확도를 측정하는 5가지 지표는 `glance()`함수를 사용하여 확인이 가능하다.  


```{r}
fit_consMR <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Production +
                          Unemployment + Savings))

```


`fabletools::glance()`함수는 모델별 fit의 결과를 요약하여 보여준다.


```{r}
glance(fit_consMR)


```


이 값들을 다른 모델의 해당 지표들과 비교하여 가장 좋은 모델을 찾는다.  
예를 들어 CV, AIC, AICc 및 BIC 측정값의 경우 가장 낮은 값을 가진 모델이 좋은 모델이며, adj_r_squared(adj $R^2$)의 경우 가장 높은 값을 가진 모델이 좋은 모델이다.  



### Adjusted $R^2$

Regression 계산 결과에는 항상 $R^2$가 출려되는데 $R^2$는 사실 모델의 예측 능력을 측정하는데 좋은 지표는 아니다.  
과거 데이터가 모델에 얼마나 잘 적합되었는지를 알 수 있는 것이지 미래 예측을 얼마나 잘 할 수 있느냐를 나타내는 것이 아니기 때문이다.   


게다가 $R^2^는 "자유도"를 고려하지 않는다. 어떤 변수를 추가하더라도 $R^2$값은 증가한다. 이러한 이유로 예측을 할 때는 $R^2$를 사용하지 않는다. 


모델을 선정하는 비슷한 방법으로 최소 오차 제곱합(SSE,  minimum sum of squared errors)가 있다.  

$$\text{SSE} = \sum_{t=1}^T e_{t}^2$$


SSE를 최소화하는 것은 $R^2$를 최대화 하는 것과 같고, 항상 가장 많은 예측변수가 있는 모델을 선택하기 때문에 예측변수를 선별하는 좋은 방법이 아니다.  


이러한 문제점을 극복하기 위해 대안으로 adjusted $R^2$(“R-bar-squared”라고 읽기도 한다.)를 사용한다.   

$$\bar{R}^2 = 1-(1-R^2)\frac{T-1}{T-k-1}$$


여기서 $T$는 관측치의 수이며 $k$는 예측변수의 수이다.  
이것은 $R^2$의 개선된 방법으로 더 이상 예측변수를 추가한다고 해서 증가하지 않는다.  


이 방법으로 $\bar{R}^2$가 최대가 되는 모델을 찾을 수 있다.  
$\bar{R}^2$가 최대가 되면 Chap7_2의 Eq (7.3)에서 표준오차$\hat{\sigma}_e$가 최소화됨을 알 수 있다.  


$\bar{R}^2$가 최대가 되는 예측변수 선정방법은 너무 많은 예측변수를 선정하는 문제점이 있지만 예측변수를 선정하는 방법으로 적절하다.





### Cross-validation

시계열의 교차검증은 Chap5_8에서 모델의 예측능력을 결정하기 위한 기본적인 도구로 소개되었다.  Regression model에서 예측변수를 선정하기 위해 고전적인 leave-one-out cross-validation 방법을 사용하는 것도 가능하다.  이것은 빠르고 데이터를 효율적으로 사용한다.  


이 방법은 아래와 같은 절차로 진행된다.  


1. 데이터 셋에서 관측치 $t$를 제거하고 남은 데이터로 모델을 적합한다. 그런 후 관측치가 제거된 상태의 오차($error = e_{t}^*=y_{t}-\hat{y}_{t}$)를 계산한다. (이것은 $t$번째 관측값을 $\hat{y}_{t}$을 예측하는데 사용하지 않기 때문에 residuals과는 다르다.)


2. 1번을 $t=1,\dots,T$동안 반복한다.  


3. $e_{1}^*,\dots,e_{T}^*$로 MSE를 계산한다. 이것을 **CV**라 한다.  


비록 이것은 시간이 많이 걸리는 방법으로 보이지만 CV를 계산하는 빠른 방법이 있어, 전체 데이터 셋에 하나의 모델을 fitting하는 것보다 오래 걸리지는 않는다.  
CV를 효율적으로 계산하는 방정식은 Chap7_9에 나와있다.  


이 기준을 적용할 때 가장 좋은 모델은 CV값이 가장 작은 모델이다.  




### Akaike’s Information Criterion

아래와 같이 정의되는 AIC(Akaike’s Information Criterion)도 하나의 방법이다.  


$$\text{AIC} = T\log\left(\frac{\text{SSE}}{T}\right) + 2(k+2)$$


여기에서 $T$는 예측을 위해 사용되는 관측치의 수이며, $k$는 모델 안에서의 예측변수 수이다.  패키지마다 AIC의 정의가 약간씩 따르다. 방정식의 $k+2$부분은 모델에 $k+2$개의 parameter (=예측변수에 대한 $k$ coefficient, 절편, 잔차분산)때문에 들어간다.  


여기에 추정해야 하는 parameter들로 모델(SSE)의 적합에 페널티를 주는 것이다. 즉 모델이 복잡할 수록 더 많은 페널피를 받는다는 의미이다. AIC는 모델 $parameter \times 2$만큼의 페널티를 부여한다.  


AIC가 작을 수록 예측의 정확도가 높다고 볼 수 있다. $T$값이 큰 경우, AIC를 최소화 하는 것은 CV값을 최소화 하는 것과 같다.




### Corrected Akaike’s Information Criterion

$T$가 작을 경우, AIC는 지나치게 많은 예측변수를 고르는 경향이 있어 bais-corrected 버전의 AIC가 개발되었다.  


$$\text{AIC}_{\text{c}} = \text{AIC} + \frac{2(k+2)(k+3)}{T-k-3}.$$

AIC와 함께 AICc도 작은 것이 좋은 것이다.  





### Schwarz’s Bayesian Information Criterion

AIC와 유사하게 일반적으로 BIC로 불리는 Schwarz’s Bayesian Information Criterion이 있다.  


$$\text{BIC} = T\log\left(\frac{\text{SSE}}{T}\right) + (k+2)\log(T)$$


BIC로 선택한 모델은 AIC에서 선택한 모델과 같거나 parameter가 더 적은 모델이다.  
이는 BIC는 모델의 $parameter \times log(T)$로 페널티를 주기 때문이다. AIC의 페널티는 표본 크기에 상관없이 일정하지만 BIC의 페널티는 표본크기($T$)가 커질수록 같이 커지기 때문에 표본 크기가 커질수록 더 큰 페널티를 부과한다.


표본크기($T$)가 큰 경우라면 BIC를 최소화하는 것은 $v = T[1-1/(\log(T)-1)]$일 때의 leave-v-out cross-validation 방법을 적용할 때와 비슷하다.  





### Which measure should we use?

$\bar{R}^2$가 널리 사용되고 다른 측정값보다 오래 사용되어 왔으나, 예측변수를 너무 많이 선택하는 경향이 있어 예측에 적합하지는 않다.  


많은 통계학자들은 BIC를 선호하는데 그 이유는 실제 기반이 되는 모델이 있는 경우 BIC가 충분한 데이터가 주어지면 해당 모델을 선택하기 때문이다.  
그러나 실제로 기반이 되는 모델은 거의 없으며 있다하더라도 그 모델을 선택하는 것이 반드시 최상의 예측을 제공하는 것은 아니다.(parameter 추정치가 정확하지 않을 수 있기 때문에)  


결과적으로 AIC, AICc, CV 중 하나를 사용하는 것이 좋다.  
표본크기($T$)가 충분히 크면 모두 동일한 모델로 이어진다.  
이 책에서는 대부분 AICc값을 사용하여 예측 모델을 선택한다.  


### Example: US consumption

US consumption을 예측하기 위한 다중회귀 예제에서 4개의 예측변수를 고려했었다.  
4개의 예측변수를 사용하면 $2^4=16$개의 가능한 모델이 생성된다. 
이제 4개의 예측변수가 모두 실제로 유용한지 또는 그 중 하나 이상을 삭제할 수 있는지 확인해본다.  

16개 모델을 모두 fit한 후의 결과는 아래와 같다.  
"O"는 예측변수가 모델에 포함되어 있음을 의미한다.  
예를 들어 첫번째 행은 4개의 예측변수가 모두 포함되었다는 것을 의미한다.  

결과는 AICc에 따라 정렬되었고 내림차순으로 정리되어 있다.  

```{r}
fit_01 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings))

rs01 <- glance(fit_01)

rs01$.model[1] <- "OOOO"

```

```{r}
fit_02 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Production + Unemployment))

rs02 <- glance(fit_02)

rs02$.model[1] <- "OOOx"

```


```{r}
fit_03 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Production + Savings))

rs03 <- glance(fit_03)

rs03$.model[1] <- "OOxO"

```


```{r}
fit_04 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Unemployment + Savings))

rs04 <- glance(fit_04)

rs04$.model[1]<- "OxOO"

```


```{r}
fit_05 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Production + Unemployment + Savings))

rs05 <- glance(fit_05)

rs05$.model[1] <- "xOOO"

```


```{r}
fit_06 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Production))

rs06 <- glance(fit_06)

rs06$.model[1] <- "OOxx"

```


```{r}
fit_07 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Unemployment))

rs07 <- glance(fit_07)

rs07$.model[1] <- "OxOx"

```


```{r}
fit_08 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income + Savings))

rs08 <- glance(fit_08)

rs08$.model[1] <- "OxxO"

```


```{r}
fit_09 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Production + Unemployment))

rs09 <- glance(fit_09)

rs09$.model[1] <- "xOOx"

```


```{r}
fit_10 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Production + Savings))

rs10 <- glance(fit_10)

rs10$.model[1] <- "xOxO"

```



```{r}
fit_11 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Unemployment + Savings))

rs11 <- glance(fit_11)

rs11$.model[1] <- "xxOO"

```


```{r}
fit_12 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Income))

rs12 <- glance(fit_12)

rs12$.model[1] <- "Oxxx"

```


```{r}
fit_13 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Production))

rs13 <- glance(fit_13)

rs13$.model[1] <- "xOxx"

```


```{r}
fit_14 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Unemployment))

rs14 <- glance(fit_14)

rs14$.model[1] <- "xxOx"

```


```{r}
fit_15 <- 
    us_change %>% 
    model(tslm = TSLM(Consumption ~ Savings))

rs15 <- glance(fit_15)

rs15$.model[1] <- "xxxO"

```


```{r}
result <- 
    bind_rows(rs01, rs02, rs03, rs04, rs05, rs06, rs07, rs08, rs09,
              rs10, rs11, rs12, rs13, rs14, rs15) %>% 
    arrange(AICc) %>% 
    select(.model, adj_r_squared, CV, AIC, AICc, BIC)

result


```


.model열은 Income, Production, Saving, Unemployment를 predictor로 적용했는지를 나타낸다.  


예상한대로 가장 좋은 결과를 보이는 모델은 예측변수 4가지가 모두 포함된 모델이다.  
결과를 보면 흥미로운 부분이 발견되는데 처음 4행과 나머지 행들과는 결과에서 큰 차이를 보인다. 


```{r}
result %>% 
    ggplot(aes(.model, AICc)) + 
    geom_bar(stat = "identity")


```


이는 Income과 Saving이 Production과 Unemployment보다 더 중요한 변수임을 보여주는 것이다.  
처음 3행의 CV, AIC, AICc값은 거의 동일하다.  
따라서 Production과 Unemployment를 없애도 유사한 수준의 예측이 가능하다.  

앞서 Fig 7.5에서 확인한 것처럼 Production과 Unemployment는 높은 음의 상관관계가 있으므로 Production에 들어있는 대부분의 예측 정보는 Unemployment에 포함된다는 점에 유의해야 한다.  




### Best subset regression

가능하면 위의 예시처럼 모든 잠재적 회귀 모델을 적합해봐야 하며, 검토했던 결과를 바탕으로 최상의 모델을 선택해야 한다.  


이것은 "best subsets" regression 또는 “all possible subsets” regression으로 알려져있다.  





### Stepwise regression

하지만 예측변수가 많으면 가능한 모든 모델을 검토해 볼 수 없다.  
예를 들어 예측변수가 40개면 $2^40=1trillion$개의 가능한 모델이 만들어진다.  
따라서 검토할 모델의 수를 제한하는 방법을 찾는게 필요하다.  

상당히 좋은 접근 방법으로 backwards stepwise regression이 있다.  


일단 모든 잠재적 예측변수를 포함하는 모델로 시작한다.  
한번에 하나씩 제거하면서 예측 정확도 측정이 향상되면 모델을 유지한다.  
더 이상 개선되지 않을 때까지 반복한다.  



잠재적 예측변수가 너무 많으면 backwards stepwise regression이 작동하지 않고 대신에 forward stepwise regression을 사용할 수도 있다.  
예측변수는 한번에 하나씩 추가되며 예측 정확도 측정을 가장 향상시키는 예측변수가 모델에 포함된다.  더 이상 개선이 이루어지지 않을 때까지 이 절차를 반복한다.  



또는 backward 혹은 forward에 대해 시작 모델은 잠재적 예측 변수를 일부를 포함하는 모델이 될 수 있다.   이 경우 추가적인 단계를 거쳐야 한다. backward인 경우에는 각 단계에 예측변수를 추가하는 것도 고려해야 하고 forward의 경우에는 예측변수를 삭제하는 것도 고려해야 한다. 이러한 것을 hybrid procedure라고 한다.  


단계적 접근 방법이 최상의 모델을 찾을 수 있다고 보장할 수 없지만 거의 항상 좋은 모델을 찾는다.  




### Beware of inference after selecting predictors

이 책에서 예측 변수의 통계적 추론에 대해서는 논의하지 않는다. (예를 들어 각 예측변수와 관련된 p-value 확인).  예측변수의 통계적 유의성을 확인하려는 경우 예측변수를 먼저 선택하는 절차를 수행하면 p-value 뒤에 있는 가정이 무효화된다.   


predictor를 선택하기 위해 권장되는 절차는 모델을 예측에 사용할 때 유용할 뿐 forecast variable에 대한 predictor의 효과를 연구하는데는 도움이 되지 않음.  











---













































































